### 决策树一般有三个重要部分，特征选择、生成、剪枝。

- 特征选择

  有这么四种标准

  构建分类树的话是信息增益、信息增益比、基尼指数

  构建回归树的话是平方误差最小化。


- 生成

  ```
  对每个特征：
  	根据某个取值（最优切分点）将数据集分成若干部分（CART是两个部分）
  		TODO
  ```

- 剪枝

  TODO

-----

### 一般有两种决策树，普通决策树和CART(分类与回归树)

- 普通决策树 - 多元切分，只能处理离散型变量

  普通决策树根据不同的特征选择算法，分为ID3算法和C4.5算法

  - ID3算法：根据信息增益作为特征选择算法

    TODO

  - C4.5算法：根据信息增益比作为特征选择算法

    信息增益比与信息增益https://www.zhihu.com/question/22928442/answer/117189907

    TODO

- CART - 二元切分，可处理连续型变量

  CART是classification and regression tree，分类与回归树，正如名字所说的，它其实有两种树，分类树和回归树。

  CART树与前面说的树有什么差别呢？

  1.之前的生成树的算法在对某个特征切分的时候，将数据集按这个特征的所有取值分成很多部分，这样的切分速度太快，而CART只进行```二元切分```，对每个特征只切分成两个部分。

  2.ID3和C4.5只能处理离散型变量，而CART因为是二元切分，```可以处理连续型变量```，而且只要将特征选择的算法改一下的话既可以生成回归树。

  - 回归树：使用平方误差最小化作为特征选择算法

    TODO

  - 分类树：使用基尼指数作为特征选择算法

  - TODO






### Chapter3

实现的是ID3算法，特征选择用的是信息增益，没有进行剪枝



### Chapter9

CART是classification and regression tree，分类与回归树，正如名字所说的，它其实有两种树，分类树和回归树。第三章中讲的决策树是ID3决策树，根据信息增益作为特征选择算法。

CART树与前面说的树有什么差别呢？

1.之前的生成树的算法在对某个特征切分的时候，将数据集按这个特征的所有取值分成很多部分，这样的切分速度太快，而CART只进行```二元切分```，对每个特征只切分成两个部分。

2.ID3和C4.5只能处理离散型变量，而CART因为是二元切分，```可以处理连续型变量```，而且只要将特征选择的算法改一下的话既可以生成回归树。

##### 回归树

- 特征选择

  回归树使用的是平方误差最小法作为特征选择算法。

  假设我们选取一个

http://blog.csdn.net/u014688145/article/details/53326910
https://www.zhihu.com/question/22697086
http://blog.csdn.net/rosenor1/article/details/52475060
https://www.zhihu.com/question/22928442
  

  ​
